import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings("ignore", category=RuntimeWarning)

# Data Preparation
data = pd.read_csv("C:/Users/rishi/Python/Turbofan_HPC_Efficiency_project 3.csv").dropna()

# Feature Scaling: Min-Max Normalization
def min_max_normalization(X):
    min_vals = np.min(X, axis=0)
    ranges = np.max(X, axis=0) - min_vals
    ranges[ranges == 0] = 1
    return (X - min_vals) / ranges

X_normalized = min_max_normalization(data.drop(columns=['Isentr.HPCEfficiency']).values)
y = data['Isentr.HPCEfficiency'].values

# Train-Test Split
def train_test_split(X, y, test_size=0.2, random_state=None):
    if random_state:
        np.random.seed(random_state)
    indices = np.arange(len(X))
    np.random.shuffle(indices)
    test_size = int(len(X) * test_size)
    return X[:-test_size], X[-test_size:], y[:-test_size], y[-test_size:]

X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)

# Ridge Regression
class RidgeRegression:
    def __init__(self, alpha=1.0):
        self.alpha = alpha

    def fit(self, X, y):
        ones = np.ones((X.shape[0], 1))
        X = np.concatenate((ones, X), axis=1)
        n_features = X.shape[1]
        self.coefficients = np.linalg.inv(X.T.dot(X) + self.alpha * np.eye(n_features)).dot(X.T).dot(y)

    def predict(self, X):
        ones = np.ones((X.shape[0], 1))
        X = np.concatenate((ones, X), axis=1)
        return X.dot(self.coefficients)

# Define and train the Ridge Regression model
alpha = 0.1  # You can adjust this hyperparameter if needed
ridge_model = RidgeRegression(alpha=alpha)
ridge_model.fit(X_train, y_train)

# Evaluate the Ridge Regression model
ridge_predictions = ridge_model.predict(X_test)
ridge_mse = np.mean((y_test - ridge_predictions) ** 2)
print("\nRidge Regression MSE:", ridge_mse)

# Neural Network for Regression
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # Initialize weights
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
    
    def sigmoid(self, x):
        return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))
    
    def sigmoid_derivative(self, x):
        return x * (1 - x)
    
    def forward(self, X):
        self.hidden_output = self.sigmoid(np.dot(X, self.weights_input_hidden))
        self.final_output = np.dot(self.hidden_output, self.weights_hidden_output)
        return self.final_output
    
    def backward(self, X, y):
        output_error = y - self.final_output
        output_delta = output_error
        hidden_error = output_delta.dot(self.weights_hidden_output.T)
        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)
        
        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * self.learning_rate
        self.weights_input_hidden += X.T.dot(hidden_delta) * self.learning_rate
    
    def train(self, X, y, epochs):
        self.training_errors = []
        self.validation_errors = []
        for epoch in range(epochs):
            self.forward(X)
            self.backward(X, y)
            if epoch % 10 == 0:  # Compute MSE every 10 epochs
                self.training_errors.append(np.mean((y_train - self.predict(X_train).flatten()) ** 2))
                self.validation_errors.append(np.mean((y_test - self.predict(X_test).flatten()) ** 2))
    
    def predict(self, X):
        return self.forward(X)

# Define and train the neural network
input_size = X_train.shape[1]
hidden_size = 10
output_size = 1
learning_rate = 0.01
epochs = 100  # Reduced number of epochs

neural_net = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)
neural_net.train(X_train, y_train.reshape(-1, 1), epochs)

# Evaluate the Neural Network
nn_predictions = neural_net.predict(X_test)
nn_mse = np.mean((y_test - nn_predictions.flatten()) ** 2)
print("\nNeural Network MSE:", nn_mse)

# Plotting MSE for Ridge Regression and Neural Network
plt.figure(figsize=(10, 6))

# Ridge Regression MSE
plt.plot([ridge_mse] * len(y_test), linestyle='--', label='Ridge Regression MSE', color='blue')

# Neural Network MSE
plt.plot([nn_mse] * len(y_test), linestyle='--', label='Neural Network MSE', color='orange')

# Print features in order from best to worst
feature_names = data.drop(columns=['Isentr.HPCEfficiency']).columns
sorted_indices = np.argsort(np.abs(ridge_model.coefficients[1:]))[::-1]  # Exclude the intercept term
print("\nFeatures in order from best to worst:")
for idx in sorted_indices:
    print(feature_names[idx])
    print("\n")

plt.xlabel('Sample Index')
plt.ylabel('MSE')
plt.title('Comparison of Mean Squared Error (MSE) between Ridge Regression and Neural Network')
plt.legend()
plt.grid(True)
plt.show()

# Plotting Predicted Efficiencies
plt.figure(figsize=(10, 6))

# Ridge Regression Predictions
plt.scatter(y_test, ridge_predictions, label='Ridge Regression', color='blue')

# Neural Network Predictions
plt.scatter(y_test, nn_predictions, label='Neural Network', color='orange')

# Plotting Diagonal Line
max_val = max(np.max(y_test), np.max(ridge_predictions), np.max(nn_predictions))
plt.plot([0, max_val], [0, max_val], linestyle='--', color='red')

plt.xlabel('Actual Efficiency')
plt.ylabel('Predicted Efficiency')
plt.title('Comparison of Actual vs Predicted Efficiency')
plt.legend()
plt.grid(True)
plt.show()

# Plotting Learning Curve for Neural Network
plt.figure(figsize=(10, 6))
plt.plot(np.arange(0, epochs, 10), neural_net.training_errors, label='Training MSE', color='blue')
plt.plot(np.arange(0, epochs, 10), neural_net.validation_errors, label='Validation MSE', color='orange')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.title('Neural Network Learning Curve')
plt.legend()
plt.grid(True)
plt.show()

# Ridge Regression Learning Curve
# Define the range of alpha values for regularization
alphas = np.logspace(-2, 2, 10)

# Arrays to store training and testing errors for each alpha
train_errors = []
test_errors = []

# Loop over different alpha values
for alpha in alphas:
    # Define and train the Ridge Regression model
    ridge_model = RidgeRegression(alpha=alpha)
    ridge_model.fit(X_train, y_train)
    
    # Predictions on training and testing data
    train_predictions = ridge_model.predict(X_train)
    test_predictions = ridge_model.predict(X_test)
    
    # Calculate MSE for training and testing data
    train_mse = np.mean((y_train - train_predictions) ** 2)
    test_mse = np.mean((y_test - test_predictions) ** 2)
    
    # Append MSE values to the arrays
    train_errors.append(train_mse)
    test_errors.append(test_mse)

# Plotting Ridge Regression Learning Curve
plt.figure(figsize=(10, 6))
plt.plot(alphas, train_errors, label='Training MSE', color='blue')
plt.plot(alphas, test_errors, label='Test MSE', color='orange')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('MSE')
plt.title('Ridge Regression Learning Curve')
plt.xscale('log')
plt.legend()
plt.grid(True)
plt.show()

# Calculate absolute coefficients for feature importance
feature_importance = np.abs(ridge_model.coefficients[1:])  # Exclude the intercept term

# Sort feature importance indices
sorted_indices = np.argsort(feature_importance)[::-1]

# Plotting Feature Importance
plt.figure(figsize=(10, 6))
plt.bar(range(len(sorted_indices)), feature_importance[sorted_indices], color='green')
plt.xlabel('Feature Index')
plt.ylabel('Absolute Coefficient')
plt.title('Feature Importance in Ridge Regression')
plt.xticks(range(len(sorted_indices)), [feature_names[idx] for idx in sorted_indices], rotation=90)
plt.tight_layout()
plt.show()
